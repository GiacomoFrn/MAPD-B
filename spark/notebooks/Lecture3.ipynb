{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb40c3f9",
   "metadata": {},
   "source": [
    "# Lecture 3: Spark Streaming\n",
    "\n",
    "_Spark Streaming_ is an extension of the Spark API that enables scalabe stream processing.\n",
    "\n",
    "The continous stream of input data can be ingested from many data sources such as **Kafka**, **Amazon s3** or **TCP sockets**. \n",
    "\n",
    "The Spark API allows to process data via high-level functions such as *map* and *reduce*. As we are going to see, it is also possible to use dataframe operations. \n",
    "\n",
    "Processed data can be exported to an external database and used to make live dashboards or offline analyses, or stored in files, or be used in a further stage of a Kafka pipeline. \n",
    "\n",
    "Spark streaming works by dividing the input data into _micro-batches_ that can be treated as static datasets. In Spark this is referred to as a *discretized stream* (*DStream*). The DStream is represented using RDDs.\n",
    "\n",
    "![DStream](imgs/lecture3/DStream.png)\n",
    "\n",
    "Any transformation applied on the DStream, i.e. anything like a `Dstream.map()`, will act independently on each batch. For example, in the image below, we can filter the original RDD to remove some data and produce a new stream. \n",
    "\n",
    "![DStream_filter](imgs/lecture3/Dstream_filter.png)\n",
    "\n",
    "In this lecture we will see how to setup a simple stream using a TCP socket as a data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77789660",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dbb40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4330629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ccf872",
   "metadata": {},
   "source": [
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc44b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231432e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 4 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14513840",
   "metadata": {},
   "source": [
    "## Create the Spark session\n",
    "\n",
    "Later on we will explain what is the role of [Apache Arrow](), but first we need to install it and create the spark session with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d10fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/bin/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/17 12:46:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"Spark streaming application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "323d6601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://dee906246066:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark streaming application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=Spark streaming application>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b253c26",
   "metadata": {},
   "source": [
    "## Spark _Streaming_ context\n",
    "\n",
    "The first step of a Spark streaming application is the creation of a `StreamingContext`. \n",
    "\n",
    "It is a similar concept to the `sparkContext` but it requires to be initialized with some additional information on how to handle the micro-batches.\n",
    "\n",
    "The context can be initialized using `StreamingContext(SparkContext, batch_interval`). \n",
    "The `batch_interval` parameter represents the wall-time between two batches, i.e. the batch duration in seconds.\n",
    "\n",
    "There could be at most one `StreamingContext` for each Spark application. \n",
    "\n",
    "The processing start when `StreamingContext.start()` is called, and it can be stopped with `StreamingContext.stop()`. \n",
    "\n",
    "By default, if the `StreamingContext` is stoped without passing the `stopSparkContext=False` option, the sparkContext is also stopped (thus the application is closed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "256cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# create a streaming context with a batch interval of 2 seconds\n",
    "ssc = StreamingContext(sc, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922cfde8",
   "metadata": {},
   "source": [
    "For this example spark will read data from a TCP socket. \n",
    "\n",
    "A dummy data stream is generated by a simple python program that can be found in `utils/producer.py`. \n",
    "When executed, the producer will write data on port `5555` of `localhost`. \n",
    "\n",
    "The dataset consists of fake credit card transactions.\n",
    "\n",
    "Have a look at the code from the producer before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab8d890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import socket\r\n",
      "import json\r\n",
      "import time\r\n",
      "import random\r\n",
      "import argparse\r\n",
      "\r\n",
      "first_names=('John','Andy','Joe','Alice')\r\n",
      "last_names=('Johnson','Smith','Jones', 'Millers')\r\n",
      "\r\n",
      "def send_messages(client_socket):\r\n",
      "    try:\r\n",
      "        while 1:\r\n",
      "            msg = {\r\n",
      "                'name': random.choice(first_names),\r\n",
      "                'surname': random.choice(last_names),\r\n",
      "                'amount': '{:.2f}'.format(random.random()*1000),\r\n",
      "                'delta_t': '{:.2f}'.format(random.random()*10),\r\n",
      "                'flag': random.choices([0,1], weights=[0.8, 0.2])[0]\r\n",
      "            }\r\n",
      "            client_socket.send((json.dumps(msg)+\"\\n\").encode('utf-8'))\r\n",
      "            time.sleep(0.1)\r\n",
      "\r\n",
      "    except KeyboardInterrupt:\r\n",
      "        exit()\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument('--hostname', type=str, required=True)\r\n",
      "    args = parser.parse_args()\r\n",
      "    print('Using hostname:', args.hostname)\r\n",
      "\r\n",
      "    new_skt = socket.socket()\r\n",
      "    host = args.hostname\r\n",
      "    port = 5555 \r\n",
      "    new_skt.bind((host, port))\r\n",
      "    print(\"Now listening on port: %s\" % str(port))\r\n",
      "\r\n",
      "    new_skt.listen(5) #  waiting for client connection.\r\n",
      "    c, addr = new_skt.accept()\r\n",
      "    print(\"Received request from: \" + str(addr))\r\n",
      "    # connection established, send messaged\r\n",
      "    send_messages(c)\r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "! cat utils/producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a375f",
   "metadata": {},
   "source": [
    "The producer will generate new records in the form of a random combination of:\n",
    "- firstname\n",
    "- lastname\n",
    "- amount\n",
    "- delta time\n",
    "- flag\n",
    "\n",
    "These information will be formatted into a .json data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d127aa0",
   "metadata": {},
   "source": [
    "To declare to Spark that the StreamingContext data source will be a TCP socket, located at a given `hostname` and `port`, we can use the following \n",
    "\n",
    "`socketTextStream(hostname, port)`\n",
    "\n",
    "Have a look at the StreamingContext documentation to see all the available options, at the [link](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.streaming.StreamingContext.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b95ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a socket stream from the hostname where we collect data from\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    hostname = \"localhost\"\n",
    "    \n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    hostname = \"spark-master\"\n",
    "\n",
    "# and set the port to 5555\n",
    "socket_stream = ssc.socketTextStream(hostname, 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad88b4",
   "metadata": {},
   "source": [
    "#### start the python producer.py script\n",
    "\n",
    "Depending on the Spark deployment mode for this exercise, do either of the following:\n",
    "\n",
    "**Local** \n",
    "- from a terminal, run the python script with the option `--hostname localhost`\n",
    "\n",
    "\n",
    "**Single Docker container**\n",
    "- from a terminal, identify the id of the container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname localhost`\n",
    "  \n",
    "**Docker cluster**\n",
    "- from a terminal, identify the id of the `spark-master` container using `docker ps`\n",
    "- connect to the Docker container using the `docker exec -it <CONTAINER_ID> bash` command\n",
    "- from the docker container, run the python script with the option `--hostname spark-master`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163466ca",
   "metadata": {},
   "source": [
    "The first thing we need to to is load the json describing each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3e7ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# use the map() transformation to apply the same function to all rdds\n",
    "# the function we want to run is the json.loads() of the messages\n",
    "json_stream = socket_stream.map(lambda msg: json.loads(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64806443",
   "metadata": {},
   "source": [
    "It is possible to print some elements of each batch with `pprint()`. This can be used to explore the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c29264",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0ad3b",
   "metadata": {},
   "source": [
    "Start the computations with `ssc.start()` and stop with `ssc.stop(stopSparkContext=False)`. Remember that after this last instruction the streaming context must be defined again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21738d69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:48\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '47.03', 'delta_t': '9.20', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '29.23', 'delta_t': '6.18', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '428.95', 'delta_t': '9.11', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '814.78', 'delta_t': '2.03', 'flag': 1}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:50\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '116.99', 'delta_t': '6.78', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '209.58', 'delta_t': '8.10', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '951.49', 'delta_t': '6.77', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '106.78', 'delta_t': '7.08', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '854.42', 'delta_t': '4.88', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '359.85', 'delta_t': '6.68', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '36.19', 'delta_t': '3.62', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '392.46', 'delta_t': '6.41', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '639.38', 'delta_t': '3.58', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '255.69', 'delta_t': '5.64', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:52\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '378.04', 'delta_t': '9.16', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '947.76', 'delta_t': '3.47', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '866.35', 'delta_t': '4.92', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '942.77', 'delta_t': '2.70', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '964.01', 'delta_t': '8.62', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '962.51', 'delta_t': '6.80', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '919.19', 'delta_t': '1.22', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '259.72', 'delta_t': '0.27', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '207.83', 'delta_t': '3.62', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '459.61', 'delta_t': '3.59', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:54\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '650.06', 'delta_t': '3.88', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '742.05', 'delta_t': '7.48', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '482.91', 'delta_t': '1.82', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '839.49', 'delta_t': '3.09', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '613.95', 'delta_t': '0.23', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '70.54', 'delta_t': '9.42', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '209.50', 'delta_t': '5.46', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '806.04', 'delta_t': '6.55', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '309.49', 'delta_t': '3.37', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '989.39', 'delta_t': '8.31', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:56\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '875.02', 'delta_t': '0.11', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '268.28', 'delta_t': '7.17', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '770.21', 'delta_t': '8.84', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '501.36', 'delta_t': '8.41', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '368.23', 'delta_t': '2.11', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '549.66', 'delta_t': '3.42', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '803.68', 'delta_t': '4.16', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '304.87', 'delta_t': '5.59', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '301.27', 'delta_t': '8.17', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '195.65', 'delta_t': '1.72', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:11:58\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '482.88', 'delta_t': '7.02', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '510.14', 'delta_t': '4.62', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '192.09', 'delta_t': '4.49', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '304.01', 'delta_t': '2.91', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '279.99', 'delta_t': '2.00', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '123.92', 'delta_t': '8.67', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '370.83', 'delta_t': '1.64', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '901.16', 'delta_t': '1.28', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '905.93', 'delta_t': '6.70', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '409.73', 'delta_t': '7.15', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:00\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '139.68', 'delta_t': '4.07', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '258.26', 'delta_t': '9.66', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '963.74', 'delta_t': '6.94', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '548.89', 'delta_t': '3.06', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '237.24', 'delta_t': '3.78', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '530.49', 'delta_t': '0.48', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '486.29', 'delta_t': '2.40', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '297.45', 'delta_t': '9.11', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '514.72', 'delta_t': '0.03', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '55.32', 'delta_t': '9.51', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:02\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '162.53', 'delta_t': '4.07', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '838.11', 'delta_t': '6.77', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '764.18', 'delta_t': '3.44', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '718.74', 'delta_t': '8.30', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '443.33', 'delta_t': '5.89', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '651.60', 'delta_t': '1.40', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '242.34', 'delta_t': '2.85', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '909.20', 'delta_t': '6.48', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '567.47', 'delta_t': '0.08', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '783.48', 'delta_t': '8.75', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:04\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '434.64', 'delta_t': '2.18', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '311.76', 'delta_t': '1.62', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '223.13', 'delta_t': '4.34', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '187.55', 'delta_t': '4.93', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '328.06', 'delta_t': '3.16', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '905.90', 'delta_t': '9.45', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '273.15', 'delta_t': '9.10', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '463.42', 'delta_t': '0.29', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '671.60', 'delta_t': '8.02', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '588.60', 'delta_t': '7.47', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:06\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '496.16', 'delta_t': '1.48', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '5.86', 'delta_t': '2.20', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '977.63', 'delta_t': '3.54', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '278.03', 'delta_t': '9.36', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '407.66', 'delta_t': '8.20', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '884.50', 'delta_t': '1.87', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '639.04', 'delta_t': '3.98', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '543.70', 'delta_t': '6.06', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '245.10', 'delta_t': '9.18', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '951.87', 'delta_t': '0.24', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:08\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '431.65', 'delta_t': '1.17', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '303.26', 'delta_t': '7.44', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '408.42', 'delta_t': '0.79', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '213.52', 'delta_t': '8.51', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '121.08', 'delta_t': '4.46', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '627.37', 'delta_t': '9.89', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '509.30', 'delta_t': '2.91', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '750.01', 'delta_t': '3.09', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '730.90', 'delta_t': '1.46', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '274.93', 'delta_t': '9.56', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:10\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '399.18', 'delta_t': '3.28', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '409.88', 'delta_t': '0.71', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '855.09', 'delta_t': '7.91', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '876.92', 'delta_t': '0.03', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '104.39', 'delta_t': '5.90', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '535.80', 'delta_t': '5.87', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '428.53', 'delta_t': '6.77', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '733.49', 'delta_t': '0.13', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '454.03', 'delta_t': '1.58', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '986.28', 'delta_t': '1.77', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:12\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '280.15', 'delta_t': '0.59', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '474.42', 'delta_t': '9.34', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '19.10', 'delta_t': '9.11', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '147.82', 'delta_t': '0.16', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '93.91', 'delta_t': '7.67', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '471.01', 'delta_t': '4.04', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '433.29', 'delta_t': '8.56', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '263.07', 'delta_t': '6.24', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '728.82', 'delta_t': '4.48', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '656.05', 'delta_t': '0.74', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:14\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '5.42', 'delta_t': '7.23', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '719.42', 'delta_t': '2.78', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '667.35', 'delta_t': '9.94', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '473.06', 'delta_t': '7.66', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '673.44', 'delta_t': '6.24', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '233.76', 'delta_t': '3.20', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '900.37', 'delta_t': '7.94', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '238.96', 'delta_t': '8.95', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '201.26', 'delta_t': '3.37', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '891.97', 'delta_t': '3.61', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:16\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '958.40', 'delta_t': '6.84', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '777.02', 'delta_t': '5.99', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '884.84', 'delta_t': '1.20', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '569.12', 'delta_t': '4.63', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '821.71', 'delta_t': '2.15', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '672.32', 'delta_t': '5.42', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '125.37', 'delta_t': '1.28', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '49.13', 'delta_t': '9.02', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '554.35', 'delta_t': '4.99', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '429.32', 'delta_t': '4.84', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:18\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '781.05', 'delta_t': '5.92', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '266.10', 'delta_t': '4.49', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '298.01', 'delta_t': '5.54', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '764.19', 'delta_t': '5.06', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '912.71', 'delta_t': '9.75', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '987.48', 'delta_t': '5.55', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '298.52', 'delta_t': '8.95', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '893.96', 'delta_t': '7.94', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '244.40', 'delta_t': '2.14', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '290.67', 'delta_t': '1.35', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:20\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '165.76', 'delta_t': '0.06', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '280.71', 'delta_t': '2.22', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '946.59', 'delta_t': '5.93', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '409.16', 'delta_t': '0.18', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '416.04', 'delta_t': '9.21', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '9.98', 'delta_t': '5.89', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '764.64', 'delta_t': '7.53', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '415.81', 'delta_t': '0.81', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '72.28', 'delta_t': '3.26', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '713.74', 'delta_t': '0.54', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:22\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '452.23', 'delta_t': '9.11', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '327.25', 'delta_t': '5.52', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '348.81', 'delta_t': '1.97', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '170.03', 'delta_t': '1.72', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '790.68', 'delta_t': '2.02', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '730.30', 'delta_t': '5.92', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '575.44', 'delta_t': '6.56', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '683.36', 'delta_t': '1.59', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '511.03', 'delta_t': '9.83', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '957.18', 'delta_t': '3.42', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:24\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '868.55', 'delta_t': '5.61', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '524.68', 'delta_t': '3.68', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '284.55', 'delta_t': '4.20', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '643.31', 'delta_t': '1.73', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '452.89', 'delta_t': '4.21', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '749.14', 'delta_t': '9.15', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '473.59', 'delta_t': '9.33', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '855.10', 'delta_t': '1.03', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '848.65', 'delta_t': '1.19', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '826.15', 'delta_t': '6.39', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:26\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '816.84', 'delta_t': '3.89', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '325.97', 'delta_t': '0.88', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '935.63', 'delta_t': '3.86', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '86.53', 'delta_t': '7.93', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '442.99', 'delta_t': '0.36', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '972.57', 'delta_t': '6.15', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '104.53', 'delta_t': '7.01', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '7.26', 'delta_t': '1.44', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '390.48', 'delta_t': '7.50', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '443.59', 'delta_t': '9.26', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:28\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '786.61', 'delta_t': '4.13', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '51.00', 'delta_t': '2.01', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '480.86', 'delta_t': '1.00', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '530.39', 'delta_t': '5.45', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '568.46', 'delta_t': '6.22', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '495.34', 'delta_t': '6.15', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '890.87', 'delta_t': '2.22', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '816.50', 'delta_t': '1.38', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '80.88', 'delta_t': '7.18', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '235.99', 'delta_t': '6.47', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:30\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '60.31', 'delta_t': '0.18', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '23.38', 'delta_t': '2.23', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '120.25', 'delta_t': '9.35', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '238.55', 'delta_t': '7.50', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '691.12', 'delta_t': '7.10', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '93.76', 'delta_t': '9.87', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '472.73', 'delta_t': '3.86', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '694.45', 'delta_t': '7.09', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '601.63', 'delta_t': '0.41', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '531.67', 'delta_t': '6.51', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:32\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '633.57', 'delta_t': '7.14', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '96.71', 'delta_t': '6.47', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '422.19', 'delta_t': '0.88', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '291.73', 'delta_t': '6.38', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '865.06', 'delta_t': '4.82', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '133.49', 'delta_t': '9.65', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '662.63', 'delta_t': '7.53', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '709.95', 'delta_t': '3.99', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '345.73', 'delta_t': '1.35', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '182.35', 'delta_t': '2.20', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:34\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '987.77', 'delta_t': '0.91', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '409.72', 'delta_t': '2.71', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '628.23', 'delta_t': '6.52', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '552.76', 'delta_t': '8.05', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '682.81', 'delta_t': '4.91', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '229.20', 'delta_t': '3.97', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '102.80', 'delta_t': '7.44', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '552.08', 'delta_t': '0.30', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '848.16', 'delta_t': '9.16', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '891.16', 'delta_t': '2.14', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:36\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '56.58', 'delta_t': '6.10', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '123.09', 'delta_t': '1.40', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '835.41', 'delta_t': '4.27', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '133.36', 'delta_t': '0.40', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '236.32', 'delta_t': '2.97', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '743.07', 'delta_t': '2.88', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '166.30', 'delta_t': '9.33', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '681.58', 'delta_t': '1.04', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '301.54', 'delta_t': '3.80', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '264.51', 'delta_t': '0.58', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:38\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '314.58', 'delta_t': '4.01', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '323.11', 'delta_t': '3.10', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '484.35', 'delta_t': '5.28', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '652.50', 'delta_t': '0.57', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '743.39', 'delta_t': '3.79', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '607.92', 'delta_t': '5.55', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '682.18', 'delta_t': '8.81', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '983.02', 'delta_t': '4.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '945.72', 'delta_t': '7.42', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '705.90', 'delta_t': '0.36', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:40\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '275.59', 'delta_t': '1.69', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '302.85', 'delta_t': '4.73', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '850.67', 'delta_t': '4.44', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '215.54', 'delta_t': '3.66', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '187.13', 'delta_t': '3.68', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '940.33', 'delta_t': '5.93', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '352.43', 'delta_t': '4.58', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '387.17', 'delta_t': '1.06', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '754.54', 'delta_t': '2.98', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '543.12', 'delta_t': '5.71', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:42\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '642.36', 'delta_t': '6.19', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '265.70', 'delta_t': '8.90', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '775.10', 'delta_t': '3.91', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '220.14', 'delta_t': '1.71', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '698.84', 'delta_t': '0.30', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '56.38', 'delta_t': '2.36', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '247.39', 'delta_t': '9.88', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '872.73', 'delta_t': '7.76', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '170.84', 'delta_t': '0.68', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '827.82', 'delta_t': '4.40', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:44\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '916.90', 'delta_t': '5.08', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '159.70', 'delta_t': '3.22', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '253.77', 'delta_t': '2.02', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '233.25', 'delta_t': '6.96', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '893.03', 'delta_t': '5.89', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '82.18', 'delta_t': '6.49', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '255.52', 'delta_t': '7.72', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '831.71', 'delta_t': '2.39', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '68.30', 'delta_t': '5.95', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '7.70', 'delta_t': '1.46', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:46\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '506.27', 'delta_t': '4.32', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '508.52', 'delta_t': '2.25', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '61.32', 'delta_t': '7.67', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '615.28', 'delta_t': '0.90', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '131.54', 'delta_t': '2.54', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '415.54', 'delta_t': '4.09', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '69.72', 'delta_t': '8.77', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '112.25', 'delta_t': '7.22', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '579.22', 'delta_t': '3.49', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '966.12', 'delta_t': '6.91', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:48\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '18.59', 'delta_t': '8.56', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '214.20', 'delta_t': '1.79', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '662.27', 'delta_t': '9.83', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '494.38', 'delta_t': '0.02', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '295.71', 'delta_t': '6.56', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '396.40', 'delta_t': '1.07', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '247.47', 'delta_t': '6.00', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '65.40', 'delta_t': '7.40', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '926.55', 'delta_t': '3.75', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '159.52', 'delta_t': '6.88', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:50\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '554.38', 'delta_t': '6.74', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '253.10', 'delta_t': '2.35', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '485.73', 'delta_t': '7.21', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '107.31', 'delta_t': '0.76', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '321.85', 'delta_t': '2.82', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '615.18', 'delta_t': '9.31', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '817.39', 'delta_t': '4.48', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '348.57', 'delta_t': '4.98', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '170.82', 'delta_t': '7.58', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '412.65', 'delta_t': '1.43', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:52\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '775.79', 'delta_t': '9.45', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '543.74', 'delta_t': '7.61', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '714.03', 'delta_t': '4.14', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '85.03', 'delta_t': '5.97', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '696.29', 'delta_t': '7.12', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '218.52', 'delta_t': '3.90', 'flag': 0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:54\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '776.08', 'delta_t': '1.40', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '391.76', 'delta_t': '7.54', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '531.64', 'delta_t': '2.62', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '886.37', 'delta_t': '5.81', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '399.34', 'delta_t': '9.41', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '861.36', 'delta_t': '7.85', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '56.43', 'delta_t': '0.29', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '477.13', 'delta_t': '1.36', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '346.70', 'delta_t': '1.75', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '660.37', 'delta_t': '7.63', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:56\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '821.94', 'delta_t': '6.73', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '909.22', 'delta_t': '6.00', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '536.51', 'delta_t': '2.43', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '228.76', 'delta_t': '1.36', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '811.16', 'delta_t': '2.49', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '144.57', 'delta_t': '9.20', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '86.10', 'delta_t': '4.38', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '826.43', 'delta_t': '0.77', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '199.01', 'delta_t': '8.16', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '911.49', 'delta_t': '2.72', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:12:58\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '413.50', 'delta_t': '3.76', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '897.12', 'delta_t': '1.99', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '417.90', 'delta_t': '2.68', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '357.88', 'delta_t': '1.76', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '316.32', 'delta_t': '6.76', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '238.57', 'delta_t': '5.10', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '880.26', 'delta_t': '4.25', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '302.81', 'delta_t': '7.20', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '989.71', 'delta_t': '5.68', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '471.24', 'delta_t': '5.46', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:00\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '630.86', 'delta_t': '2.83', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '561.39', 'delta_t': '2.10', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '401.06', 'delta_t': '4.95', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '569.54', 'delta_t': '5.00', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '357.26', 'delta_t': '0.14', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '396.81', 'delta_t': '7.98', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '402.79', 'delta_t': '0.93', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '501.23', 'delta_t': '9.91', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '360.57', 'delta_t': '5.64', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '765.53', 'delta_t': '7.91', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:02\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '902.17', 'delta_t': '6.44', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '253.39', 'delta_t': '7.51', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '614.76', 'delta_t': '4.87', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '908.85', 'delta_t': '7.65', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '248.46', 'delta_t': '9.84', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '852.62', 'delta_t': '7.14', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '938.74', 'delta_t': '5.67', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '917.74', 'delta_t': '9.92', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '36.33', 'delta_t': '8.75', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '951.96', 'delta_t': '4.70', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:04\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '758.81', 'delta_t': '8.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '401.77', 'delta_t': '6.72', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '850.30', 'delta_t': '6.52', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '180.13', 'delta_t': '3.16', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '892.43', 'delta_t': '4.96', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '534.14', 'delta_t': '7.89', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '755.00', 'delta_t': '1.71', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '805.96', 'delta_t': '1.26', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '518.01', 'delta_t': '6.38', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '874.00', 'delta_t': '3.21', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:06\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '28.73', 'delta_t': '6.19', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '19.26', 'delta_t': '6.77', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '316.11', 'delta_t': '5.08', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '891.32', 'delta_t': '4.62', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '346.71', 'delta_t': '2.13', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '587.79', 'delta_t': '5.08', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '332.02', 'delta_t': '3.13', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '218.78', 'delta_t': '7.32', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '640.80', 'delta_t': '8.18', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '385.37', 'delta_t': '0.52', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:08\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '895.05', 'delta_t': '6.48', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '536.34', 'delta_t': '4.77', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '389.39', 'delta_t': '8.29', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '749.18', 'delta_t': '4.27', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '179.90', 'delta_t': '1.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '451.24', 'delta_t': '9.69', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '21.56', 'delta_t': '2.54', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '230.43', 'delta_t': '5.76', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '785.78', 'delta_t': '2.07', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '840.45', 'delta_t': '2.94', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:10\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '889.13', 'delta_t': '6.44', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '808.83', 'delta_t': '6.25', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '190.04', 'delta_t': '4.38', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '78.22', 'delta_t': '8.77', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '894.10', 'delta_t': '7.31', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '372.84', 'delta_t': '4.24', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '743.00', 'delta_t': '8.98', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '976.27', 'delta_t': '2.38', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '715.22', 'delta_t': '2.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '633.74', 'delta_t': '6.11', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:12\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '402.40', 'delta_t': '0.03', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '9.78', 'delta_t': '5.61', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '161.82', 'delta_t': '9.23', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '146.24', 'delta_t': '3.44', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '902.35', 'delta_t': '3.51', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '96.70', 'delta_t': '6.41', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '161.11', 'delta_t': '5.57', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '799.75', 'delta_t': '8.94', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '202.77', 'delta_t': '3.63', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '379.78', 'delta_t': '1.33', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:14\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '638.19', 'delta_t': '8.32', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '850.48', 'delta_t': '3.45', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '130.86', 'delta_t': '2.43', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '386.46', 'delta_t': '4.73', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '232.69', 'delta_t': '5.02', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '406.30', 'delta_t': '3.18', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '448.63', 'delta_t': '4.52', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '42.21', 'delta_t': '7.94', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '14.03', 'delta_t': '6.69', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '108.12', 'delta_t': '6.26', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:16\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '310.34', 'delta_t': '9.84', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '505.62', 'delta_t': '6.94', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '783.88', 'delta_t': '7.09', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '185.83', 'delta_t': '6.42', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '425.46', 'delta_t': '6.68', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '258.31', 'delta_t': '7.86', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '336.37', 'delta_t': '9.23', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '923.26', 'delta_t': '8.34', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '405.63', 'delta_t': '2.49', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '825.78', 'delta_t': '6.13', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:18\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '620.16', 'delta_t': '8.75', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '497.86', 'delta_t': '2.71', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '414.20', 'delta_t': '7.65', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '113.33', 'delta_t': '0.02', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '266.07', 'delta_t': '9.93', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '291.44', 'delta_t': '9.59', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '881.46', 'delta_t': '9.99', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '116.35', 'delta_t': '5.70', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '769.26', 'delta_t': '7.39', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '646.52', 'delta_t': '1.14', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:20\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '644.30', 'delta_t': '0.61', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '861.94', 'delta_t': '2.86', 'flag': 0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:22\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '812.81', 'delta_t': '8.38', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '561.57', 'delta_t': '6.00', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '878.45', 'delta_t': '6.64', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '862.62', 'delta_t': '3.22', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '440.60', 'delta_t': '6.50', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '989.25', 'delta_t': '0.45', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '784.52', 'delta_t': '8.12', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '82.89', 'delta_t': '7.44', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '168.79', 'delta_t': '9.59', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '272.98', 'delta_t': '3.80', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:24\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '395.70', 'delta_t': '1.08', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '585.15', 'delta_t': '4.78', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '1.15', 'delta_t': '6.54', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '930.79', 'delta_t': '1.16', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '212.82', 'delta_t': '9.29', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '715.11', 'delta_t': '8.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '829.86', 'delta_t': '0.01', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '389.84', 'delta_t': '8.30', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '89.30', 'delta_t': '0.28', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '473.89', 'delta_t': '8.73', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:26\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '791.59', 'delta_t': '1.22', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '43.72', 'delta_t': '8.74', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '850.45', 'delta_t': '7.85', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '965.71', 'delta_t': '0.76', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '37.87', 'delta_t': '8.12', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '622.66', 'delta_t': '5.12', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '65.74', 'delta_t': '0.00', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '35.23', 'delta_t': '4.17', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '385.06', 'delta_t': '10.00', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '847.53', 'delta_t': '4.68', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:28\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '627.13', 'delta_t': '2.83', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '322.73', 'delta_t': '7.56', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '20.67', 'delta_t': '6.58', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '783.41', 'delta_t': '8.03', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '665.93', 'delta_t': '6.94', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '246.14', 'delta_t': '8.41', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '273.67', 'delta_t': '1.73', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '297.20', 'delta_t': '4.67', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '60.55', 'delta_t': '6.29', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '512.53', 'delta_t': '8.07', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:30\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '435.30', 'delta_t': '9.01', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '263.01', 'delta_t': '8.80', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '794.06', 'delta_t': '4.26', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '643.07', 'delta_t': '9.50', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '459.10', 'delta_t': '6.38', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '809.72', 'delta_t': '0.79', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '745.36', 'delta_t': '6.65', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '852.96', 'delta_t': '7.96', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '294.73', 'delta_t': '4.13', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '870.35', 'delta_t': '2.88', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:32\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '369.15', 'delta_t': '0.71', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '896.78', 'delta_t': '5.89', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '595.26', 'delta_t': '3.37', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '319.41', 'delta_t': '2.37', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '322.63', 'delta_t': '6.79', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '57.16', 'delta_t': '1.29', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '983.71', 'delta_t': '8.11', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '218.99', 'delta_t': '6.82', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '483.08', 'delta_t': '9.19', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '931.11', 'delta_t': '3.11', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:34\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '388.53', 'delta_t': '5.75', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '142.49', 'delta_t': '4.10', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '67.11', 'delta_t': '4.17', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '744.83', 'delta_t': '7.67', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '708.76', 'delta_t': '0.94', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '425.15', 'delta_t': '7.86', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '285.04', 'delta_t': '7.29', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '865.10', 'delta_t': '1.31', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '821.79', 'delta_t': '4.03', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '107.36', 'delta_t': '3.46', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:36\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '165.68', 'delta_t': '4.79', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '571.92', 'delta_t': '5.03', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '771.48', 'delta_t': '3.12', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '177.31', 'delta_t': '1.46', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '817.96', 'delta_t': '3.76', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '810.99', 'delta_t': '7.55', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '175.44', 'delta_t': '4.95', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '939.38', 'delta_t': '1.79', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '389.65', 'delta_t': '3.29', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '370.77', 'delta_t': '3.02', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:38\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '123.54', 'delta_t': '3.03', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '1.13', 'delta_t': '8.88', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '812.88', 'delta_t': '8.51', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '71.96', 'delta_t': '8.30', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '210.93', 'delta_t': '9.74', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '203.47', 'delta_t': '9.56', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '22.95', 'delta_t': '1.74', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '483.94', 'delta_t': '7.83', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '723.20', 'delta_t': '7.33', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '653.28', 'delta_t': '6.48', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:40\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '750.54', 'delta_t': '8.33', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '267.89', 'delta_t': '5.49', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '609.82', 'delta_t': '4.86', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '130.95', 'delta_t': '1.47', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '956.52', 'delta_t': '5.62', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '559.40', 'delta_t': '6.88', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '382.89', 'delta_t': '2.11', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '921.17', 'delta_t': '9.61', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '814.37', 'delta_t': '0.87', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '714.31', 'delta_t': '8.91', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:42\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '283.51', 'delta_t': '2.52', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '407.04', 'delta_t': '0.37', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '609.30', 'delta_t': '9.49', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '278.07', 'delta_t': '2.43', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '646.74', 'delta_t': '8.14', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '538.88', 'delta_t': '3.80', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '727.52', 'delta_t': '7.21', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '438.35', 'delta_t': '4.13', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '485.89', 'delta_t': '4.32', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '4.64', 'delta_t': '6.55', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:44\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '497.06', 'delta_t': '5.69', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '232.95', 'delta_t': '5.94', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '759.71', 'delta_t': '5.20', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '832.39', 'delta_t': '1.15', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '856.37', 'delta_t': '0.50', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '770.73', 'delta_t': '8.95', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '131.47', 'delta_t': '4.75', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '59.78', 'delta_t': '9.72', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '118.52', 'delta_t': '4.50', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '584.91', 'delta_t': '6.38', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:46\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '346.48', 'delta_t': '8.89', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '804.66', 'delta_t': '9.76', 'flag': 0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:48\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '508.35', 'delta_t': '3.71', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '5.34', 'delta_t': '3.71', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '721.62', 'delta_t': '3.40', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '7.98', 'delta_t': '6.33', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '792.30', 'delta_t': '6.35', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '211.85', 'delta_t': '2.42', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '931.40', 'delta_t': '2.20', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '74.84', 'delta_t': '4.15', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '367.37', 'delta_t': '0.43', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '23.22', 'delta_t': '5.39', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:50\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '285.54', 'delta_t': '4.05', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '284.57', 'delta_t': '5.89', 'flag': 0}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:52\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '427.28', 'delta_t': '4.98', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '924.32', 'delta_t': '9.04', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '553.76', 'delta_t': '4.77', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '721.58', 'delta_t': '5.14', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '358.00', 'delta_t': '0.66', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '781.56', 'delta_t': '0.66', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '445.82', 'delta_t': '4.35', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '572.22', 'delta_t': '4.70', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '97.57', 'delta_t': '3.26', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '801.05', 'delta_t': '3.76', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:54\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '984.97', 'delta_t': '2.78', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '618.48', 'delta_t': '3.98', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '906.31', 'delta_t': '5.55', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '880.40', 'delta_t': '2.19', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '329.78', 'delta_t': '2.22', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '613.94', 'delta_t': '0.68', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '205.14', 'delta_t': '3.59', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '842.46', 'delta_t': '9.37', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '268.53', 'delta_t': '2.79', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '891.03', 'delta_t': '6.72', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:56\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '270.26', 'delta_t': '9.75', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '92.95', 'delta_t': '1.16', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '740.30', 'delta_t': '8.18', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '50.39', 'delta_t': '5.62', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '4.76', 'delta_t': '6.87', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '299.13', 'delta_t': '2.22', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '827.30', 'delta_t': '8.99', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '57.51', 'delta_t': '9.26', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '606.30', 'delta_t': '4.08', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '601.75', 'delta_t': '6.50', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:13:58\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '37.82', 'delta_t': '1.87', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '468.31', 'delta_t': '8.28', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '175.90', 'delta_t': '9.09', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '219.45', 'delta_t': '2.42', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '104.54', 'delta_t': '4.08', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '474.25', 'delta_t': '1.29', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '779.32', 'delta_t': '6.11', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '934.89', 'delta_t': '0.31', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '564.34', 'delta_t': '6.54', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '155.82', 'delta_t': '2.53', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:00\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '701.87', 'delta_t': '5.28', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '493.13', 'delta_t': '4.57', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '225.51', 'delta_t': '5.94', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '662.85', 'delta_t': '9.59', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '318.67', 'delta_t': '3.15', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '316.44', 'delta_t': '1.50', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '882.35', 'delta_t': '2.90', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '476.00', 'delta_t': '1.55', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '806.67', 'delta_t': '2.71', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '588.70', 'delta_t': '2.20', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:02\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '52.88', 'delta_t': '5.40', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '11.77', 'delta_t': '8.97', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '48.02', 'delta_t': '0.53', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '708.32', 'delta_t': '1.61', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '36.35', 'delta_t': '2.34', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '656.85', 'delta_t': '1.99', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '277.79', 'delta_t': '5.64', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '920.17', 'delta_t': '0.63', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '828.79', 'delta_t': '4.20', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '817.01', 'delta_t': '8.18', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:04\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '879.90', 'delta_t': '0.29', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '682.27', 'delta_t': '6.83', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '884.88', 'delta_t': '6.27', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '78.36', 'delta_t': '6.16', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '641.24', 'delta_t': '1.03', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '288.01', 'delta_t': '4.26', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '176.03', 'delta_t': '5.71', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '230.37', 'delta_t': '9.36', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '389.76', 'delta_t': '6.53', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '252.91', 'delta_t': '4.30', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:06\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '672.68', 'delta_t': '1.00', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '819.72', 'delta_t': '1.53', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '437.35', 'delta_t': '1.10', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '944.35', 'delta_t': '3.47', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '429.46', 'delta_t': '2.32', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '581.93', 'delta_t': '1.19', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '546.99', 'delta_t': '1.87', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '658.84', 'delta_t': '3.53', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '394.82', 'delta_t': '0.27', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '104.96', 'delta_t': '6.39', 'flag': 1}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:08\n",
      "-------------------------------------------\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '953.20', 'delta_t': '7.15', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '675.23', 'delta_t': '0.14', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '571.42', 'delta_t': '7.23', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '180.83', 'delta_t': '6.00', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '53.17', 'delta_t': '9.47', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '791.27', 'delta_t': '8.48', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '547.35', 'delta_t': '8.88', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '600.65', 'delta_t': '2.83', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '874.47', 'delta_t': '9.40', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '165.54', 'delta_t': '4.03', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:10\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '406.01', 'delta_t': '4.69', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '91.22', 'delta_t': '4.95', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '731.97', 'delta_t': '5.49', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '882.81', 'delta_t': '6.53', 'flag': 0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:12\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '879.90', 'delta_t': '1.65', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '112.92', 'delta_t': '1.53', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '968.03', 'delta_t': '1.37', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '806.12', 'delta_t': '3.03', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '328.13', 'delta_t': '4.67', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '255.81', 'delta_t': '4.84', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '659.08', 'delta_t': '0.12', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '970.22', 'delta_t': '4.11', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '427.21', 'delta_t': '8.98', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '163.41', 'delta_t': '6.69', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:14\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '350.57', 'delta_t': '6.81', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '869.53', 'delta_t': '9.71', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '291.67', 'delta_t': '6.76', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '80.80', 'delta_t': '9.54', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '648.79', 'delta_t': '8.87', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '367.04', 'delta_t': '5.16', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '389.11', 'delta_t': '9.79', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '377.51', 'delta_t': '8.88', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '753.73', 'delta_t': '8.60', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '669.78', 'delta_t': '8.54', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:16\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '441.54', 'delta_t': '2.98', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '372.28', 'delta_t': '0.92', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '331.00', 'delta_t': '3.20', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '243.01', 'delta_t': '7.15', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '577.20', 'delta_t': '4.68', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Millers', 'amount': '865.32', 'delta_t': '4.12', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '305.19', 'delta_t': '3.97', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '849.29', 'delta_t': '3.82', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '18.26', 'delta_t': '1.64', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '897.80', 'delta_t': '4.34', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:18\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '302.00', 'delta_t': '2.02', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '479.40', 'delta_t': '0.83', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '857.25', 'delta_t': '5.89', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '954.56', 'delta_t': '6.49', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '681.60', 'delta_t': '6.84', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '878.66', 'delta_t': '2.07', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '893.65', 'delta_t': '7.42', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '770.39', 'delta_t': '8.01', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '319.25', 'delta_t': '4.08', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '413.87', 'delta_t': '5.69', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:20\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '842.86', 'delta_t': '7.33', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '431.42', 'delta_t': '7.12', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '234.84', 'delta_t': '9.63', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '10.99', 'delta_t': '8.00', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '440.26', 'delta_t': '1.18', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '67.64', 'delta_t': '2.36', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '691.52', 'delta_t': '8.21', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '496.05', 'delta_t': '5.59', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '358.05', 'delta_t': '8.66', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '843.23', 'delta_t': '8.95', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:22\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Smith', 'amount': '233.09', 'delta_t': '4.51', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '122.54', 'delta_t': '9.04', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '710.26', 'delta_t': '0.61', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '220.08', 'delta_t': '9.81', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '933.83', 'delta_t': '7.85', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '997.50', 'delta_t': '5.27', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '931.36', 'delta_t': '0.92', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '480.02', 'delta_t': '5.10', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '190.42', 'delta_t': '7.42', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '899.30', 'delta_t': '2.81', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:24\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '931.81', 'delta_t': '7.69', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '958.07', 'delta_t': '2.91', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '97.78', 'delta_t': '2.66', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '5.72', 'delta_t': '3.71', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '542.80', 'delta_t': '0.32', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '361.76', 'delta_t': '7.19', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '789.88', 'delta_t': '9.68', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '424.46', 'delta_t': '1.07', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '473.33', 'delta_t': '0.38', 'flag': 1}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '206.98', 'delta_t': '4.36', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:26\n",
      "-------------------------------------------\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '985.99', 'delta_t': '6.46', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '119.27', 'delta_t': '2.49', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '137.20', 'delta_t': '0.11', 'flag': 1}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '792.63', 'delta_t': '9.68', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '21.75', 'delta_t': '2.49', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '488.26', 'delta_t': '4.46', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '916.80', 'delta_t': '2.16', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '633.39', 'delta_t': '4.29', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '906.91', 'delta_t': '8.31', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Millers', 'amount': '265.96', 'delta_t': '10.00', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:28\n",
      "-------------------------------------------\n",
      "{'name': 'John', 'surname': 'Johnson', 'amount': '148.62', 'delta_t': '3.20', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Millers', 'amount': '876.58', 'delta_t': '4.58', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '454.61', 'delta_t': '7.22', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '258.64', 'delta_t': '0.90', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Smith', 'amount': '776.44', 'delta_t': '3.87', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '960.35', 'delta_t': '0.67', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Smith', 'amount': '878.45', 'delta_t': '8.88', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '821.54', 'delta_t': '3.89', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '334.43', 'delta_t': '2.56', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '127.73', 'delta_t': '3.26', 'flag': 0}\n",
      "...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3b52cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:14:32 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:30\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '885.67', 'delta_t': '6.89', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '711.12', 'delta_t': '8.61', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Johnson', 'amount': '932.67', 'delta_t': '2.51', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '645.74', 'delta_t': '4.88', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Jones', 'amount': '367.22', 'delta_t': '9.26', 'flag': 0}\n",
      "{'name': 'John', 'surname': 'Jones', 'amount': '920.15', 'delta_t': '4.80', 'flag': 0}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '946.23', 'delta_t': '7.00', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Johnson', 'amount': '533.97', 'delta_t': '4.99', 'flag': 0}\n",
      "{'name': 'Andy', 'surname': 'Jones', 'amount': '358.90', 'delta_t': '7.69', 'flag': 0}\n",
      "{'name': 'Joe', 'surname': 'Smith', 'amount': '746.40', 'delta_t': '4.40', 'flag': 0}\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2022-05-17 13:14:32\n",
      "-------------------------------------------\n",
      "{'name': 'Alice', 'surname': 'Jones', 'amount': '93.31', 'delta_t': '8.10', 'flag': 1}\n",
      "{'name': 'Alice', 'surname': 'Johnson', 'amount': '568.31', 'delta_t': '5.70', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '405.76', 'delta_t': '5.60', 'flag': 1}\n",
      "{'name': 'Andy', 'surname': 'Millers', 'amount': '445.58', 'delta_t': '6.56', 'flag': 0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:14:34 ERROR TaskSchedulerImpl: Lost executor 1 on 172.21.0.4: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7dc2a8",
   "metadata": {},
   "source": [
    "Now that we know we can stream data to Spark we would like to start performing basic operations in a distributed fashion.\n",
    "\n",
    "Once the SparkStreamingContext is stopped we have to re-create a new one, as the connection between the socket and Spark is lost.\n",
    "\n",
    "Before running the following cells we should therefore:\n",
    "1. create a new `scc` object\n",
    "2. point it to the proper TCP socket and port\n",
    "3. start again the python producer application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1c082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Spark StreamingContext with a batch wall-time of 2 seconds\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c9ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the socket stream using the appropriate endpoint and port\n",
    "socket_stream = ssc.socketTextStream(hostname, 5555)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894abd4",
   "metadata": {},
   "source": [
    "#### start once again the python producer script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86587a4a",
   "metadata": {},
   "source": [
    "We now start listening on the TCP socket, interpreting the input data stream as json loads.\n",
    "\n",
    "Remember to get rid of the `pprint()` action, that would otherwise be performed continously, dumping the input data into the Jupyter cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a86004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json_stream object by reading the json loads from the socket\n",
    "json_stream = socket_stream.map(lambda msg: json.loads(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ab834",
   "metadata": {},
   "source": [
    "We may want to convert each batch into a Spark DataFrame to have access to the higher level APIs. \n",
    "\n",
    "In order to do that, let's first convert all the numeric features of the json into python floats and integers variables. \n",
    "This is a simple type cast operation in python, that can be easily parallelized.\n",
    "\n",
    "The python dictionary produced by each json message, re-casted with the proper data types, can then be used to create a `Row`for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0782d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# create a row for each message \n",
    "#   convert each numerical value to the proper python type\n",
    "#   create a row from each message\n",
    "def create_row_rdd(t):\n",
    "    t['amount'] = float(t['amount'])\n",
    "    t['delta t'] = float(t['delta t'])\n",
    "    t['flag'] = int(t['flag'])\n",
    "    return Row(**t)\n",
    "\n",
    "# apply the transformation to the json_stream rdd\n",
    "row_stream = json_stream.map(create_row_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceaa05f",
   "metadata": {},
   "source": [
    "The method `DStream.foreachRDD` can be used to apply custom transformations to each *batch* of data. \n",
    "\n",
    "In our case, we are insterested in converting each batch of data into a Spark dataframe and perform operations, such as finding the number of transactions for each user. \n",
    "\n",
    "For the scope of this simple use-case we can consider that all batches where a user has performed more than one transaction with the `flag` field equal to one can be identified as fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7224636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if left unconstrained, Spark might want to create a very \n",
    "# large number of partitions for this streaming application\n",
    "# \n",
    "# using way more partitions than necessary always results in\n",
    "# a huge over-head due to the partition-to-partition communications\n",
    "# \n",
    "# this line is a trick to force Spark to use a small number of partitions\n",
    "# thus making it more efficient in the case of small workloads and few executors\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81cbd75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit, countDistinct\n",
    "\n",
    "# process each bach to identify possibly fraudolent transactions\n",
    "#   1. convert the rdd into a dataframe (provide the schema if necessary)\n",
    "#   2. compute the number of transactions per batch per user (a unique combination of first_and_lastname)\n",
    "#   3. identify all the \"suspicios\" transactions by counting the number of transactions from a unique user \n",
    "#   3. print the resulting dataframe to mimick a live reporting of the suspicious transactions\n",
    "\n",
    "def process_batch(rdd):\n",
    "    # convert rdd to df\n",
    "    #   check the documentation and/or the Lecture2 notebook \n",
    "    #   for details on how to create and pass a schema to a dataframe   \n",
    "    df = rdd.toDF(\n",
    "        schema='name string, surname string, mount float, delta_t, float, flag_int'\n",
    "    )\n",
    "    \n",
    "    # find number of transactions for each user when flag = 1 \n",
    "    #    declare a new column to create a unique user identifier \n",
    "    #    this can be easily done by concatenating first- and last-name fields\n",
    "    #    check the concat function from pyspark.sql.functions \n",
    "    num_transactions = (\n",
    "        df\n",
    "        .where(col('flag')==1)\n",
    "        .withColumn('id', concat(col('name'), col('surname')))\n",
    "        .groupBy('id')\n",
    "        .count()\n",
    "    )\n",
    "    \n",
    "    # find suspicious transactions\n",
    "    #    filter only users with more than one transaction per batch\n",
    "    #    create a \"fraud\" column with a value of 1 for the selected users (check the lit function)\n",
    "    #    from the dataframe, project only the unique id and fraud columns\n",
    "    sus_transactions = (\n",
    "        num_transactions\n",
    "        .where(col('count')>1)\n",
    "        .withColumn('fraud', lit(1))\n",
    "        .select(col('id'), col('fraud'))\n",
    "    )\n",
    "    \n",
    "    # (trigger an automatic alert)\n",
    "    # print the first 5 items of the resulting dataframe\n",
    "    sus_transactions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a7758",
   "metadata": {},
   "source": [
    "Finally, instruct Spark to execute this `process_batch` function for each RDD you will have in your DStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b911a910",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_stream.foreachRDD(process_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad010131",
   "metadata": {},
   "source": [
    "Now you should be ready to start the spark streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07b9ee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:29 ERROR JobScheduler: Error running job streaming job 1652795006000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:29 ERROR JobScheduler: Error running job streaming job 1652795008000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:30 ERROR JobScheduler: Error running job streaming job 1652795010000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:32 ERROR JobScheduler: Error running job streaming job 1652795012000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:34 ERROR JobScheduler: Error running job streaming job 1652795014000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:36 ERROR JobScheduler: Error running job streaming job 1652795016000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:38 ERROR JobScheduler: Error running job streaming job 1652795018000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:40 ERROR JobScheduler: Error running job streaming job 1652795020000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:42 ERROR JobScheduler: Error running job streaming job 1652795022000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:44 ERROR JobScheduler: Error running job streaming job 1652795024000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:46 ERROR JobScheduler: Error running job streaming job 1652795026000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:48 ERROR JobScheduler: Error running job streaming job 1652795028000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:50 ERROR JobScheduler: Error running job streaming job 1652795030000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:52 ERROR JobScheduler: Error running job streaming job 1652795032000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:54 ERROR JobScheduler: Error running job streaming job 1652795034000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:43:56 ERROR JobScheduler: Error running job streaming job 1652795036000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:43:58 ERROR JobScheduler: Error running job streaming job 1652795038000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:00 ERROR JobScheduler: Error running job streaming job 1652795040000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:02 ERROR JobScheduler: Error running job streaming job 1652795042000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:04 ERROR JobScheduler: Error running job streaming job 1652795044000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:06 ERROR JobScheduler: Error running job streaming job 1652795046000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:08 ERROR JobScheduler: Error running job streaming job 1652795048000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:10 ERROR JobScheduler: Error running job streaming job 1652795050000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:12 ERROR JobScheduler: Error running job streaming job 1652795052000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:14 ERROR JobScheduler: Error running job streaming job 1652795054000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:17 ERROR JobScheduler: Error running job streaming job 1652795056000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:18 ERROR JobScheduler: Error running job streaming job 1652795058000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:20 ERROR JobScheduler: Error running job streaming job 1652795060000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:22 ERROR JobScheduler: Error running job streaming job 1652795062000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:24 ERROR JobScheduler: Error running job streaming job 1652795064000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:26 ERROR JobScheduler: Error running job streaming job 1652795066000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:28 ERROR JobScheduler: Error running job streaming job 1652795068000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:30 ERROR JobScheduler: Error running job streaming job 1652795070000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:32 ERROR JobScheduler: Error running job streaming job 1652795072000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:34 ERROR JobScheduler: Error running job streaming job 1652795074000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:36 ERROR JobScheduler: Error running job streaming job 1652795076000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:38 ERROR JobScheduler: Error running job streaming job 1652795078000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:40 ERROR JobScheduler: Error running job streaming job 1652795080000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:42 ERROR JobScheduler: Error running job streaming job 1652795082000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:44 ERROR JobScheduler: Error running job streaming job 1652795084000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:46 ERROR JobScheduler: Error running job streaming job 1652795086000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:48 ERROR JobScheduler: Error running job streaming job 1652795088000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:50 ERROR JobScheduler: Error running job streaming job 1652795090000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:52 ERROR JobScheduler: Error running job streaming job 1652795092000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:54 ERROR JobScheduler: Error running job streaming job 1652795094000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:44:56 ERROR JobScheduler: Error running job streaming job 1652795096000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:44:58 ERROR JobScheduler: Error running job streaming job 1652795098000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:00 ERROR JobScheduler: Error running job streaming job 1652795100000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:02 ERROR JobScheduler: Error running job streaming job 1652795102000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:04 ERROR JobScheduler: Error running job streaming job 1652795104000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:06 ERROR JobScheduler: Error running job streaming job 1652795106000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:08 ERROR JobScheduler: Error running job streaming job 1652795108000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:10 ERROR JobScheduler: Error running job streaming job 1652795110000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:12 ERROR JobScheduler: Error running job streaming job 1652795112000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:14 ERROR JobScheduler: Error running job streaming job 1652795114000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:16 ERROR JobScheduler: Error running job streaming job 1652795116000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:18 ERROR JobScheduler: Error running job streaming job 1652795118000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:20 ERROR JobScheduler: Error running job streaming job 1652795120000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:22 ERROR JobScheduler: Error running job streaming job 1652795122000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:24 ERROR JobScheduler: Error running job streaming job 1652795124000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:26 ERROR JobScheduler: Error running job streaming job 1652795126000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:28 ERROR JobScheduler: Error running job streaming job 1652795128000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:30 ERROR JobScheduler: Error running job streaming job 1652795130000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:32 ERROR JobScheduler: Error running job streaming job 1652795132000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:34 ERROR JobScheduler: Error running job streaming job 1652795134000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:36 ERROR JobScheduler: Error running job streaming job 1652795136000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:38 ERROR JobScheduler: Error running job streaming job 1652795138000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:40 ERROR JobScheduler: Error running job streaming job 1652795140000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:42 ERROR JobScheduler: Error running job streaming job 1652795142000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:44 ERROR JobScheduler: Error running job streaming job 1652795144000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:46 ERROR JobScheduler: Error running job streaming job 1652795146000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:48 ERROR JobScheduler: Error running job streaming job 1652795148000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:50 ERROR JobScheduler: Error running job streaming job 1652795150000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:52 ERROR JobScheduler: Error running job streaming job 1652795152000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:54 ERROR JobScheduler: Error running job streaming job 1652795154000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:45:56 ERROR JobScheduler: Error running job streaming job 1652795156000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:45:58 ERROR JobScheduler: Error running job streaming job 1652795158000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:00 ERROR JobScheduler: Error running job streaming job 1652795160000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:02 ERROR JobScheduler: Error running job streaming job 1652795162000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:04 ERROR JobScheduler: Error running job streaming job 1652795164000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:06 ERROR JobScheduler: Error running job streaming job 1652795166000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:08 ERROR JobScheduler: Error running job streaming job 1652795168000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:10 ERROR JobScheduler: Error running job streaming job 1652795170000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:12 ERROR JobScheduler: Error running job streaming job 1652795172000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:14 ERROR JobScheduler: Error running job streaming job 1652795174000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:16 ERROR JobScheduler: Error running job streaming job 1652795176000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:18 ERROR JobScheduler: Error running job streaming job 1652795178000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:20 ERROR JobScheduler: Error running job streaming job 1652795180000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:22 ERROR JobScheduler: Error running job streaming job 1652795182000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:24 ERROR JobScheduler: Error running job streaming job 1652795184000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:26 ERROR JobScheduler: Error running job streaming job 1652795186000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:28 ERROR JobScheduler: Error running job streaming job 1652795188000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:30 ERROR JobScheduler: Error running job streaming job 1652795190000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:32 ERROR JobScheduler: Error running job streaming job 1652795192000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:34 ERROR JobScheduler: Error running job streaming job 1652795194000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/17 13:46:36 ERROR JobScheduler: Error running job streaming job 1652795196000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:38 ERROR JobScheduler: Error running job streaming job 1652795198000 ms.0\n",
      "org.apache.spark.SparkException: An exception was raised by Python:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/util.py\", line 68, in call\n",
      "    r = self.func(t, *rdds)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n",
      "    func = lambda t, rdd: old_func(rdd)\n",
      "  File \"/tmp/ipykernel_11/1232840701.py\", line 13, in process_batch\n",
      "    df = rdd.toDF(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 66, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/session.py\", line 661, in createDataFrame\n",
      "    schema = _parse_datatype_string(schema)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 843, in _parse_datatype_string\n",
      "    raise e\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 833, in _parse_datatype_string\n",
      "    return from_ddl_schema(s)\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/types.py\", line 825, in from_ddl_schema\n",
      "    sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/bin/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.ParseException: \n",
      "extraneous input ',' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 49)\n",
      "\n",
      "== SQL ==\n",
      "name string, surname string, mount float, delta_t, float, flag_int\n",
      "-------------------------------------------------^^^\n",
      "\n",
      "\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n",
      "\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n",
      "\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# start streaming context\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b834a208",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/17 13:46:38 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/05/17 13:46:39 ERROR TaskSchedulerImpl: Lost executor 0 on 172.21.0.3: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/17 13:46:39 WARN BlockManagerMasterEndpoint: No more replicas available for input-0-1652793270600 !\n",
      "22/05/17 13:46:39 WARN BlockManagerMasterEndpoint: No more replicas available for input-0-1652793270800 !\n",
      "22/05/17 13:46:39 WARN BlockManagerMasterEndpoint: No more replicas available for input-0-1652793269800 !\n"
     ]
    }
   ],
   "source": [
    "# stop streaming context - no not stop the SparkContext\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d3e64",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e1abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
